{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Style Transfer with TensorFlow and Keras\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/abrampers/real-time-style-transfer-tensorflow/blob/master/Real%20Time%20Style%20Transfer%20-%20TensorFlow.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/abrampers/real-time-style-transfer-tensorflow/blob/master/Real%20Time%20Style%20Transfer%20-%20TensorFlow.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll implement a network that performs __neural style transfer__ based on paper by [Justin Johnson, et al](https://cs.stanford.edu/people/jcjohns/eccv16/).\n",
    ">[Justin Johnson's paper](https://cs.stanford.edu/people/jcjohns/eccv16/) states that using this method is giving similar qualitative results but is three orders of magnitude faster than optimization technique outlined in [Leon A. Gatys' paper, A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576).\n",
    "\n",
    "## Overview\n",
    "Neural style transfer is an optimization technique used to take three images, a __content__ image, a __style reference__ image (such as an artwork by a famous painter), and the input image you want to style -- and blend them together such that the input image is transformed to look like the content image, but “painted” in the style of the style image.\n",
    "\n",
    "In this paper, style transfer is done by training a deep convolutional neural network using a pretrained deep convolutional neural network. In this case, we're using [VGG16](https://arxiv.org/abs/1409.1556) pretrained on imagenet dataset.\n",
    "\n",
    "# TODO: masukin gambar arsitektur networknya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS:\n",
    "1. Create keras.Layers class instead of functions\n",
    "2. Search for style images\n",
    "\n",
    "### List of style images\n",
    "1. starry night\n",
    "2. hockney\n",
    "3. monet\n",
    "4. rain princess\n",
    "5. the scream\n",
    "6. udnie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resources\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Content and Style images\n",
    "Here, we create function to load image and do VGG16 standard preprocessing using `tf.keras.applications.vgg16.preprocess_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, target_size=None):\n",
    "    \"\"\" Load image from path and do VGG16 standard preprocessing with tf mode.\n",
    "        Returns a tensor representation of the image.\n",
    "    \"\"\"\n",
    "    if target_size is None:\n",
    "        image = tf.keras.preprocessing.image.load_img(image_path)\n",
    "    else:\n",
    "        image = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size)\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = tf.keras.applications.vgg16.preprocess_input(image, mode='tf')\n",
    "    image = tf.convert_to_tensor(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to implement the function to load image from the MS COCO dataset given the image path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_image(image_path):\n",
    "    \"\"\" Mapping function to load train image from path\n",
    "    \"\"\"\n",
    "    img = tf.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize_images(img, (480, 640))\n",
    "    image_shape = tf.shape(img)\n",
    "    img = tf.reshape(img, [image_shape[0], image_shape[1], image_shape[2]])\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img, mode='tf')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a few helper to show and save the image from a `tf.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, title=None, denormalize=True):\n",
    "    \"\"\" Showing image tensor\n",
    "    \"\"\"\n",
    "    image = image.numpy()\n",
    "    if denormalize:\n",
    "        image = (image + 1.) * 127.5\n",
    "    # Remove the batch dimension\n",
    "    image = np.squeeze(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.imshow(image/255.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image, image_path, denormalize=True):\n",
    "    \"\"\" Save image from tensor\n",
    "    \"\"\"\n",
    "    if denormalize:\n",
    "        image = (image + 1) * 127.5\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    image = tf.squeeze(image, axis=0)\n",
    "    image = tf.image.encode_jpeg(image)\n",
    "    tf.io.write_file(image_path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're loading the content image by the name of the file and show the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_image = load_image(\"images/content/mug.jpg\")\n",
    "imshow(content_image)\n",
    "content_shape = tf.shape(content_image)\n",
    "_, img_height, img_width, _ = content_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to load and show the style image and force the style image to match the size of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = load_image(\"images/styles/wave_crop.jpg\", target_size=(img_height, img_width))\n",
    "imshow(style_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Training images\n",
    "Here, we load [Microsoft's COCO dataset](https://arxiv.org/pdf/1405.0312.pdf) for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_zip = 'train2014.zip'\n",
    "name_of_folder = 'train2014'\n",
    "if not os.path.exists(os.path.abspath('.') + '/' + name_of_folder):\n",
    "    image_zip = tf.keras.utils.get_file(name_of_zip, \n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "    mscoco_path = os.path.dirname(image_zip)+'/train2014/'\n",
    "else:\n",
    "    mscoco_path = os.path.abspath('.')+'/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchsize = 4\n",
    "mscoco_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames of the training images\n",
    "mscoco_filenames = tf.constant(mscoco_path) + os.listdir('train2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a `tf.data.Dataset` filenames\n",
    "dataset = tf.data.Dataset.from_tensor_slices(mscoco_filenames)\n",
    "# Load all the data from filenames\n",
    "dataset = dataset.map(load_train_image)\n",
    "# Create dataset with train_batchsize\n",
    "dataset = dataset.batch(train_batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the data is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and show the first batch of the dataset\n",
    "batch_image = next(iter(dataset))\n",
    "for image in batch_image:\n",
    "    imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network with TensorFlow\n",
    "Below is where we'll define the network as in the paper of [Justin Johnson, et al](https://cs.stanford.edu/people/jcjohns/eccv16/).\n",
    "\n",
    "<img src=\"images/assets/model_architecture.png\" width=1000px>\n",
    "\n",
    "Next, we'll use TensorFlow to define the architecture of the network. We start by defining the layers and operations we want. Then, define a method for the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_padding():\n",
    "    \"\"\"Reflection padding layer for output size to match the input size\"\"\"\n",
    "    def f(inputs):\n",
    "        return tf.pad(inputs, [[0, 0], [40, 40], [40, 40], [0, 0]], \"REFLECT\")\n",
    "    return f\n",
    "\n",
    "def conv_layer(n_channels, kernel_size, strides, padding=\"same\", relu=True):\n",
    "    \"\"\"Convolutional layer wrapper\"\"\"\n",
    "    def f(inputs):\n",
    "        conv = tf.keras.layers.Conv2D(filters=n_channels, \n",
    "                                      kernel_size=kernel_size, \n",
    "                                      strides=strides, \n",
    "                                      padding=padding)(inputs)\n",
    "        bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "        if relu:\n",
    "            return tf.keras.layers.Activation(\"relu\")(bn)\n",
    "        else:\n",
    "            return bn\n",
    "        \n",
    "    return f\n",
    "\n",
    "def conv_transpose_layer(n_channels, kernel_size, strides, padding=\"same\", relu=True):\n",
    "    \"\"\"Convolutional transpose layer to upsample the image\"\"\"\n",
    "    def f(inputs):\n",
    "        conv = tf.keras.layers.Conv2DTranspose(n_channels, \n",
    "                                               kernel_size=kernel_size, \n",
    "                                               strides=strides, \n",
    "                                               padding=padding)(inputs)\n",
    "        bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "        if relu:\n",
    "            return tf.keras.layers.Activation(\"relu\")(bn)\n",
    "        else:\n",
    "            return bn\n",
    "        \n",
    "    return f\n",
    "\n",
    "def residual_block(n_channels, kernel_size=3, strides=1, padding='valid'):\n",
    "    \"\"\"Residual Block. Center cropped the input to match output size\"\"\"\n",
    "    def f(inputs):\n",
    "        inputs_shape = inputs.get_shape().as_list()\n",
    "        residual = tf.image.resize_image_with_crop_or_pad(inputs, inputs_shape[1] - 4, inputs_shape[2] - 4)\n",
    "        conv_1 = tf.keras.layers.Conv2D(filters=n_channels, \n",
    "                                        kernel_size=kernel_size, \n",
    "                                        strides=strides, \n",
    "                                        padding=padding)(inputs)\n",
    "        bn_1 = tf.keras.layers.BatchNormalization()(conv_1)\n",
    "        relu_1 = tf.keras.layers.Activation(\"relu\")(bn_1)\n",
    "        conv_2 = tf.keras.layers.Conv2D(filters=n_channels, \n",
    "                                        kernel_size=kernel_size, \n",
    "                                        strides=strides, \n",
    "                                        padding=padding)(relu_1)\n",
    "        bn_2 = tf.keras.layers.BatchNormalization()(conv_2)\n",
    "        return tf.keras.layers.add([bn_2, residual])\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferModel(tf.keras.Model):\n",
    "    \"\"\"Style Transfer Model class\"\"\"\n",
    "    def __init__(self):\n",
    "        super(StyleTransferModel, self).__init__(name='style_transfer_model')\n",
    "        \n",
    "        # Layers\n",
    "        self.pad = reflection_padding()\n",
    "        self.conv_1 = conv_layer(32, 9, 1)\n",
    "        self.conv_2 = conv_layer(64, 3, 2)\n",
    "        self.conv_3 = conv_layer(128, 3, 2)\n",
    "        self.res_1 = residual_block(128, 3, 1)\n",
    "        self.res_2 = residual_block(128, 3, 1)\n",
    "        self.res_3 = residual_block(128, 3, 1)\n",
    "        self.res_4 = residual_block(128, 3, 1)\n",
    "        self.res_5 = residual_block(128, 3, 1)\n",
    "        self.conv_4 = conv_transpose_layer(64, 3, 2)\n",
    "        self.conv_5 = conv_transpose_layer(32, 3, 2)\n",
    "        self.conv_6 = conv_layer(3, 9, 1, relu=False)\n",
    "\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # (width x height x channels)\n",
    "        # 256 x 256 x 3\n",
    "\n",
    "        # 336 x 336 x 3\n",
    "        padded = self.pad(inputs)\n",
    "        # 336 x 336 x 32\n",
    "        conv_1_out = self.conv_1(padded)\n",
    "        # 168 x 64 x 64\n",
    "        conv_2_out = self.conv_2(conv_1_out)\n",
    "        # 84 x 84 x 128\n",
    "        conv_3_out = self.conv_3(conv_2_out)\n",
    "        # 80 x 80 x 128\n",
    "        res_1_out = self.res_1(conv_3_out)\n",
    "        # 76 x 76 x 128\n",
    "        res_2_out = self.res_2(res_1_out)\n",
    "        # 72 x 72 x 128\n",
    "        res_3_out = self.res_3(res_2_out)\n",
    "        # 68 x 68 x 128\n",
    "        res_4_out = self.res_4(res_3_out)\n",
    "        # 64 x 64 x 128\n",
    "        res_5_out = self.res_5(res_4_out)\n",
    "        # 128 x 128 x 64\n",
    "        conv_4_out = self.conv_4(res_5_out)\n",
    "        # 256 x 256 x 32\n",
    "        conv_5_out = self.conv_5(conv_4_out)\n",
    "        # 256 x 256 x 3\n",
    "        conv_6_out = self.conv_6(conv_5_out)\n",
    "        \n",
    "        tanh_out = (conv_6_out + 1) * 255. / 2\n",
    "        return tanh_out\n",
    "    \n",
    "    def preprocess_input(x):\n",
    "        \"\"\" Denormalizes input\n",
    "        \"\"\"\n",
    "        return (x / 127.5) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = StyleTransferModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try forward pass the content_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = net(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_features(model, image):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of style layers.\n",
    "        Returns a dictionary of the layer name and the activations.\n",
    "    \"\"\"\n",
    "    style_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3']\n",
    "    \n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for layer in model.layers:\n",
    "        x = layer(x)\n",
    "        if layer.name in style_layers:\n",
    "            features[layer.name] = x\n",
    "            if layer.name == 'block4_conv3':\n",
    "                break\n",
    "            \n",
    "    return features\n",
    "\n",
    "def get_content_feature(model, image):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of conent layers.\n",
    "        Returns the activation of the content layer\n",
    "    \"\"\"\n",
    "    style_layers = ['block3_conv3']\n",
    "    \n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for layer in model.layers:\n",
    "        x = layer(x)\n",
    "        if layer.name in style_layers:\n",
    "            features = x\n",
    "            break\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    \"\"\" Compute gram matrix of a 3 dimensional convolution\n",
    "    \"\"\"\n",
    "    b, h, w, c = tf.shape(x)\n",
    "    x = tf.reshape(x, [b, c, -1])\n",
    "    size = tf.to_float(c * h * w)\n",
    "    return  tf.matmul(x, tf.transpose(x, perm=[0, 2, 1])) / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_WEIGHT = 7.5e0\n",
    "STYLE_WEIGHT = 1e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, content_image, style_image, content_weight, style_weight):\n",
    "    \"\"\" Compute loss of output with respect to content and style image\n",
    "    \"\"\"\n",
    "    # Pretrained VGG16 on imagenet\n",
    "    model = tf.keras.applications.vgg16.VGG16()\n",
    "    \n",
    "    # Style features of output\n",
    "    output_style_features = get_style_features(model, y)\n",
    "    # Content features of output\n",
    "    output_content_feature = get_content_feature(model, y)\n",
    "    \n",
    "    # Style features of style image\n",
    "    style_features = get_style_features(model, style_image)\n",
    "    # Content features of content image\n",
    "    content_feature = get_content_feature(model, content_image)\n",
    "    \n",
    "    # Compute content loss\n",
    "    # (output - content )/(Cj * Hj * Wj)\n",
    "    content_loss = content_weight * tf.reduce_mean(tf.math.square(output_content_feature - content_feature))\n",
    "    \n",
    "    # Compute style loss\n",
    "    # Gram matrix of output features\n",
    "    output_grams = [gram_matrix(x) for _, x in output_style_features.items()]\n",
    "                                                  \n",
    "    # Gram matrix of style features\n",
    "    style_grams = [gram_matrix(x) for _, x in style_features.items()]\n",
    "    \n",
    "    style_losses = [tf.square(tf.norm(output_gram - style_gram)) for output_gram, style_gram in zip(output_grams, style_grams)]\n",
    "    style_loss = style_weight * tf.reduce_sum(tf.convert_to_tensor(style_losses)) / 4.\n",
    "                                                  \n",
    "    # TODO: Add total variation regularization\n",
    "    \n",
    "    total_loss = content_loss + style_loss\n",
    "    return total_loss, content_loss, style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tf.Tensor: id=18761, shape=(), dtype=float32, numpy=4519431000000.0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tf.Tensor: id=12538, shape=(), dtype=float32, numpy=2355534000000.0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(y_hat, content_image, style_image, 1, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
