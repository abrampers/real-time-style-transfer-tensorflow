{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Style Transfer with TensorFlow and Keras\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/abrampers/real-time-style-transfer-tensorflow/blob/master/Real%20Time%20Style%20Transfer%20-%20TensorFlow.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/abrampers/real-time-style-transfer-tensorflow/blob/master/Real%20Time%20Style%20Transfer%20-%20TensorFlow.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll implement a network that performs __neural style transfer__ based on paper by [Justin Johnson, et al](https://cs.stanford.edu/people/jcjohns/eccv16/).\n",
    ">[Justin Johnson's paper](https://cs.stanford.edu/people/jcjohns/eccv16/) states that using this method is giving similar qualitative results but is three orders of magnitude faster than optimization technique outlined in [Leon A. Gatys' paper, A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576).\n",
    "\n",
    "## Overview\n",
    "Neural style transfer is an optimization technique used to take three images, a __content__ image, a __style reference__ image (such as an artwork by a famous painter), and the input image you want to style -- and blend them together such that the input image is transformed to look like the content image, but “painted” in the style of the style image.\n",
    "\n",
    "In this paper, style transfer is done by training a deep convolutional neural network using a pretrained deep convolutional neural network. In this case, we're using [VGG16](https://arxiv.org/abs/1409.1556) pretrained on imagenet dataset.\n",
    "\n",
    "# TODO: masukin gambar arsitektur networknya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS:\n",
    "1. Create keras.Layers class instead of functions\n",
    "2. Search for style images\n",
    "\n",
    "### List of style images\n",
    "1. starry night\n",
    "2. hockney\n",
    "3. monet\n",
    "4. rain princess\n",
    "5. the scream\n",
    "6. udnie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resources\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Content and Style images\n",
    "Here, we create function to load image and do VGG16 standard preprocessing using `tf.keras.applications.vgg16.preprocess_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, target_size=None):\n",
    "    \"\"\" Load image from path and do VGG16 standard preprocessing with tf mode.\n",
    "        Returns a tensor representation of the image.\n",
    "    \"\"\"\n",
    "    image = tf.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    if target_size is not None:\n",
    "        image = tf.image.resize_images(image, target_size)\n",
    "    image_shape = tf.shape(image)\n",
    "    image = tf.reshape(image, [1, image_shape[0], image_shape[1], image_shape[2]])\n",
    "    return tf.to_float(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to implement the function to load image from the MS COCO dataset given the image path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_image(image_path):\n",
    "    \"\"\" Mapping function to load train image from path\n",
    "    \"\"\"\n",
    "    img = tf.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize_images(img, (480, 640))\n",
    "    image_shape = tf.shape(img)\n",
    "    img = tf.reshape(img, [image_shape[0], image_shape[1], image_shape[2]])\n",
    "    return tf.to_float(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a few helper to show and save the image from a `tf.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, title=None):\n",
    "    \"\"\" Showing image tensor\n",
    "    \"\"\"\n",
    "    image = image.numpy()\n",
    "    # Remove the batch dimension\n",
    "    image = np.squeeze(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.imshow(image/255.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image, image_path):\n",
    "    \"\"\" Save image from tensor\n",
    "    \"\"\"\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    image = tf.squeeze(image, axis=0)\n",
    "    image = tf.image.encode_jpeg(image)\n",
    "    tf.io.write_file(image_path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're loading the content image by the name of the file and show the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_image = load_image(\"image/content/mug.jpg\")\n",
    "imshow(content_image)\n",
    "content_shape = tf.shape(content_image)\n",
    "_, img_height, img_width, _ = content_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to load and show the style image and force the style image to match the size of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = load_image(\"image/style/wave_crop.jpg\", target_size=(img_height, img_width))\n",
    "imshow(style_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Training images\n",
    "Here, we load [Microsoft's COCO dataset](https://arxiv.org/pdf/1405.0312.pdf) for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_zip = 'train2014.zip'\n",
    "name_of_folder = 'train2014'\n",
    "if not os.path.exists(os.path.abspath('.') + '/' + name_of_folder):\n",
    "    image_zip = tf.keras.utils.get_file(name_of_zip, \n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "    mscoco_path = os.path.dirname(image_zip)+'/train2014/'\n",
    "else:\n",
    "    mscoco_path = os.path.abspath('.')+'/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchsize = 4\n",
    "mscoco_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames of the training images\n",
    "mscoco_filenames = tf.constant(mscoco_path) + os.listdir('train2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a `tf.data.Dataset` filenames\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(mscoco_filenames)\n",
    "# Load all the data from filenames\n",
    "train_dataset = train_dataset.map(load_train_image)\n",
    "# Create dataset with train_batchsize\n",
    "train_dataset = train_dataset.batch(train_batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the data is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and show the first batch of the dataset\n",
    "batch_image = next(iter(train_dataset))\n",
    "for image in batch_image:\n",
    "    imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network with TensorFlow\n",
    "Below is where we'll define the network as in the paper of [Justin Johnson, et al](https://cs.stanford.edu/people/jcjohns/eccv16/).\n",
    "\n",
    "<img src=\"image/assets/model_architecture.png\" width=800px>\n",
    "\n",
    "Next, we'll use TensorFlow to define the architecture of the network. We start by defining the layers and operations we want. Then, define a method for the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the layers\n",
    "Before defining the model, we define the layers as the building blocks of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, n_channels, kernel_size=3, strides=1, padding='valid'):\n",
    "        super(ResidualBlock, self).__init__(name='residual_block')\n",
    "        self.n_channels = n_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=self.n_channels, \n",
    "                                      kernel_size=self.kernel_size, \n",
    "                                      strides=self.strides, \n",
    "                                      padding=self.padding)\n",
    "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.Activation(\"relu\")\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=self.n_channels, \n",
    "                                      kernel_size=self.kernel_size, \n",
    "                                      strides=self.strides, \n",
    "                                      padding=self.padding)\n",
    "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        _, h, w, _ = inputs.get_shape().as_list()\n",
    "        \n",
    "        # Downsample the residual to match output of the convolution\n",
    "        residual = tf.image.resize_image_with_crop_or_pad(inputs, h - 4, w - 4)\n",
    "        \n",
    "        conv_1 = self.conv1(inputs)\n",
    "        bn_1 = self.batch_norm1(conv_1)\n",
    "        relu_1 = self.relu1(bn_1)\n",
    "        conv_2 = self.conv2(relu_1)\n",
    "        bn_2 = self.batch_norm2(conv_2)\n",
    "        return tf.keras.layers.add([bn_2, residual])\n",
    "\n",
    "class ReflectionPadding2D(tf.keras.Model):\n",
    "    \"\"\"Reflection padding for output size to match the input size\"\"\"\n",
    "    def __init__(self, horizontal_pad, vertical_pad):\n",
    "        super(ReflectionPadding2D, self).__init__(name='reflection_padding')\n",
    "        self.h_pad = horizontal_pad\n",
    "        self.v_pad = vertical_pad\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        return tf.pad(inputs, [[0, 0], \n",
    "                               [self.h_pad, self.h_pad], \n",
    "                               [self.v_pad, self.v_pad], \n",
    "                               [0, 0]], \"REFLECT\")\n",
    "    \n",
    "\n",
    "def reflection_padding():\n",
    "    \"\"\"Reflection padding for output size to match the input size\"\"\"\n",
    "    def f(inputs):\n",
    "        return tf.pad(inputs, [[0, 0], [40, 40], [40, 40], [0, 0]], \"REFLECT\")\n",
    "    return f\n",
    "\n",
    "class ConvolutionBlockLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Convolutional block\"\"\"\n",
    "    def __init__(self, n_channels, kernel_size, strides, padding='same', relu=True):\n",
    "        super(ConvolutionBlockLayer, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.do_relu = relu\n",
    "        \n",
    "        # Layers\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=self.n_channels, \n",
    "                                      kernel_size=self.kernel_size, \n",
    "                                      strides=self.strides, \n",
    "                                      padding=self.padding)\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.Activation(\"relu\")\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        conv = self.conv(inputs)\n",
    "        bn = self.batch_norm(conv)\n",
    "        if self.do_relu:\n",
    "            return self.relu(bn)\n",
    "        else:\n",
    "            return bn\n",
    "\n",
    "class ConvolutionBlock(tf.keras.Model):\n",
    "    \"\"\"Convolutional block\"\"\"\n",
    "    def __init__(self, n_channels, kernel_size, strides, padding='same', relu=True):\n",
    "        super(ConvolutionBlock, self).__init__(name='conv_block')\n",
    "        self.n_channels = n_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.do_relu = relu\n",
    "        \n",
    "        # Layers\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=self.n_channels, \n",
    "                                      kernel_size=self.kernel_size, \n",
    "                                      strides=self.strides, \n",
    "                                      padding=self.padding)\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.Activation(\"relu\")\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        conv = self.conv(inputs)\n",
    "        bn = self.batch_norm(conv)\n",
    "        if self.do_relu:\n",
    "            return self.relu(bn)\n",
    "        else:\n",
    "            return bn\n",
    "\n",
    "def conv_layer(n_channels, kernel_size, strides, padding=\"same\", relu=True):\n",
    "    \"\"\"Convolutional block\"\"\"\n",
    "    def f(inputs):\n",
    "        conv = tf.layers.Conv2D(filters=n_channels, \n",
    "                                      kernel_size=kernel_size, \n",
    "                                      strides=strides, \n",
    "                                      padding=padding,\n",
    "                               trainable=True)(inputs)\n",
    "        bn = tf.layers.BatchNormalization()(conv)\n",
    "        if relu:\n",
    "            return tf.keras.layers.Activation(\"relu\")(bn)\n",
    "        else:\n",
    "            return bn\n",
    "        \n",
    "    return f\n",
    "\n",
    "class ConvolutionTransposeBlock(tf.keras.Model):\n",
    "    \"\"\"Convolutional Transpose block\"\"\"\n",
    "    def __init__(self, n_channels, kernel_size, strides, padding='same', relu=True):\n",
    "        super(ConvolutionTransposeBlock, self).__init__(name='conv_transpose_block')\n",
    "        self.n_channels = n_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.do_relu = relu\n",
    "        \n",
    "        # Layers\n",
    "        self.conv_t = tf.keras.layers.Conv2DTranspose(filters=self.n_channels, \n",
    "                                      kernel_size=self.kernel_size, \n",
    "                                      strides=self.strides, \n",
    "                                      padding=self.padding)\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.Activation(\"relu\")\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        conv = self.conv_t(inputs)\n",
    "        bn = self.batch_norm(conv)\n",
    "        if self.do_relu:\n",
    "            return self.relu(bn)\n",
    "        else:\n",
    "            return bn\n",
    "\n",
    "def conv_transpose_layer(n_channels, kernel_size, strides, padding=\"same\", relu=True):\n",
    "    \"\"\"Convolutional transpose layer to upsample the image\"\"\"\n",
    "    def f(inputs):\n",
    "        conv = tf.layers.Conv2DTranspose(n_channels, \n",
    "                                               kernel_size=kernel_size, \n",
    "                                               strides=strides, \n",
    "                                               padding=padding)(inputs)\n",
    "        bn = tf.layers.BatchNormalization()(conv)\n",
    "        if relu:\n",
    "            return tf.keras.layers.Activation(\"relu\")(bn)\n",
    "        else:\n",
    "            return bn\n",
    "        \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual block\n",
    "In this network, we'll use residual blocks as in the paper by [Deep residual learning for image recognition by He, et al.]()\n",
    "\n",
    "However the residual blocks we are going to implement will follow the architecture outlined in [Gross, et al.]()\n",
    "\n",
    "<img src=\"image/assets/residual_block_architecture.png\" width=\"150px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, n_channels, kernel_size=3, strides=1, padding='valid'):\n",
    "        super(ResidualBlock, self).__init__(name='style_transfer_model')\n",
    "        self.n_channels = n_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=self.n_channels, \n",
    "                                      kernel_size=self.kernel_size, \n",
    "                                      strides=self.strides, \n",
    "                                      padding=self.padding)\n",
    "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.Activation(\"relu\")\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=self.n_channels, \n",
    "                                      kernel_size=self.kernel_size, \n",
    "                                      strides=self.strides, \n",
    "                                      padding=self.padding)\n",
    "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        _, h, w, _ = inputs.get_shape().as_list()\n",
    "        \n",
    "        # Downsample the residual to match output of the convolution\n",
    "        residual = tf.image.resize_image_with_crop_or_pad(inputs, h - 4, w - 4)\n",
    "        \n",
    "        conv_1 = self.conv1(inputs)\n",
    "        bn_1 = self.batch_norm1(conv_1)\n",
    "        relu_1 = self.relu1(bn_1)\n",
    "        conv_2 = self.conv2(relu_1)\n",
    "        bn_2 = self.batch_norm2(conv_2)\n",
    "        return tf.keras.layers.add([bn_2, residual])\n",
    "    \n",
    "class ResizeImage(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ResizeImage, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        _, h, w, _ = inputs.get_shape().as_list()\n",
    "        residual = tf.image.resize_image_with_crop_or_pad(inputs, h - 4, w - 4)\n",
    "        return residual\n",
    "        \n",
    "def residual_block(n_channels, kernel_size=3, strides=1, padding='valid'):\n",
    "    \"\"\"Residual Block. Center cropped the input to match output size\"\"\"\n",
    "    def f(inputs):\n",
    "        _, h, w, _ = inputs.get_shape().as_list()\n",
    "        \n",
    "        # Downsample the residual to match output of the convolution\n",
    "        residual = tf.image.resize_image_with_crop_or_pad(inputs, h - 4, w - 4)\n",
    "        \n",
    "        conv_1 = tf.layers.Conv2D(filters=n_channels, \n",
    "                                        kernel_size=kernel_size, \n",
    "                                        strides=strides, \n",
    "                                        padding=padding)(inputs)\n",
    "        bn_1 = tf.layers.BatchNormalization()(conv_1)\n",
    "        relu_1 = tf.keras.layers.Activation(\"relu\")(bn_1)\n",
    "        conv_2 = tf.layers.Conv2D(filters=n_channels, \n",
    "                                        kernel_size=kernel_size, \n",
    "                                        strides=strides, \n",
    "                                        padding=padding)(relu_1)\n",
    "        bn_2 = tf.layers.BatchNormalization()(conv_2)\n",
    "        return tf.keras.layers.add([bn_2, residual])\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Image Transform Network\n",
    "We've created all the necessary layers, now it's time to implement the actual image transformation layer!\n",
    "\n",
    "<img src=\"image/assets/model_specification.png\" width=\"300px\">\n",
    "\n",
    "Here's the exact model architecture based on [Justin Johnson's supplementary material](https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16Supplementary.pdf) that we're going to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferModel(tf.keras.Model):\n",
    "    \"\"\"Style Transfer Model class\"\"\"\n",
    "    def __init__(self):\n",
    "        super(StyleTransferModel, self).__init__(name='style_transfer_model')\n",
    "        \n",
    "        # Layers\n",
    "        self.pad = ReflectionPadding2D(40, 40)\n",
    "        self.conv_1 = ConvolutionBlock(32, 9, 1)\n",
    "        self.conv_2 = ConvolutionBlock(64, 3, 2)\n",
    "        self.conv_3 = ConvolutionBlock(128, 3, 2)\n",
    "        self.res_1 = ResidualBlock(128, 3, 1)\n",
    "        self.res_2 = ResidualBlock(128, 3, 1)\n",
    "        self.res_3 = ResidualBlock(128, 3, 1)\n",
    "        self.res_4 = ResidualBlock(128, 3, 1)\n",
    "        self.res_5 = ResidualBlock(128, 3, 1)\n",
    "        self.conv_4 = ConvolutionTransposeBlock(64, 3, 2)\n",
    "        self.conv_5 = ConvolutionTransposeBlock(32, 3, 2)\n",
    "        self.conv_6 = ConvolutionBlock(3, 9, 1, relu=False)\n",
    "\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # (width x height x channels)\n",
    "        \n",
    "        # 336 x 336 x 3\n",
    "        padded = self.pad(inputs)\n",
    "        # 336 x 336 x 32\n",
    "        conv_1_out = self.conv_1(padded)\n",
    "        # 168 x 64 x 64\n",
    "        conv_2_out = self.conv_2(conv_1_out)\n",
    "        # 84 x 84 x 128\n",
    "        conv_3_out = self.conv_3(conv_2_out)\n",
    "        # 80 x 80 x 128\n",
    "        res_1_out = self.res_1(conv_3_out)\n",
    "        # 76 x 76 x 128\n",
    "        res_2_out = self.res_2(res_1_out)\n",
    "        # 72 x 72 x 128\n",
    "        res_3_out = self.res_3(res_2_out)\n",
    "        # 68 x 68 x 128\n",
    "        res_4_out = self.res_4(res_3_out)\n",
    "        # 64 x 64 x 128\n",
    "        res_5_out = self.res_5(res_4_out)\n",
    "        # 128 x 128 x 64\n",
    "        conv_4_out = self.conv_4(res_5_out)\n",
    "        # 256 x 256 x 32\n",
    "        conv_5_out = self.conv_5(conv_4_out)\n",
    "        # 256 x 256 x 3\n",
    "        conv_6_out = self.conv_6(conv_5_out)\n",
    "        tanh_out = tf.tanh(conv_6_out)\n",
    "        out = (tanh_out + 1) * 255. / 2\n",
    "        return out\n",
    "    \n",
    "    def preprocess_input(x):\n",
    "        \"\"\" Denormalizes input\n",
    "        \"\"\"\n",
    "        return (x / 127.5) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_padding():\n",
    "    \"\"\"Reflection padding for output size to match the input size\"\"\"\n",
    "    def f(inputs):\n",
    "        return tf.pad(inputs, [[40, 40], [40, 40], [0, 0]], \"REFLECT\")\n",
    "    return f\n",
    "\n",
    "class StyleTransferModel1:\n",
    "    \"\"\"Style Transfer Model class\"\"\"\n",
    "    def __init__(self, input_shape):\n",
    "        \n",
    "        if len(input_shape) != 3:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_cols, nb_rows, nb_channels)\")\n",
    "            \n",
    "        # Layers\n",
    "        self.input = tf.keras.layers.Input(shape=input_shape)\n",
    "        self.pad = reflection_padding()\n",
    "        self.conv_1 = conv_layer(32, 9, 1)\n",
    "        self.conv_2 = conv_layer(64, 3, 2)\n",
    "        self.conv_3 = conv_layer(128, 3, 2)\n",
    "        self.res_1 = residual_block(128, 3, 1)\n",
    "        self.res_2 = residual_block(128, 3, 1)\n",
    "        self.res_3 = residual_block(128, 3, 1)\n",
    "        self.res_4 = residual_block(128, 3, 1)\n",
    "        self.res_5 = residual_block(128, 3, 1)\n",
    "        self.conv_4 = conv_transpose_layer(64, 3, 2)\n",
    "        self.conv_5 = conv_transpose_layer(32, 3, 2)\n",
    "        self.conv_6 = conv_layer(3, 9, 1, relu=False)\n",
    "        \n",
    "        # (width x height x channels)\n",
    "        inputs = self.input\n",
    "        # 336 x 336 x 3\n",
    "        padded = self.pad(inputs)\n",
    "        # 336 x 336 x 32\n",
    "        conv_1_out = self.conv_1(inputs)\n",
    "        # 168 x 64 x 64\n",
    "        conv_2_out = self.conv_2(conv_1_out)\n",
    "        # 84 x 84 x 128\n",
    "        conv_3_out = self.conv_3(conv_2_out)\n",
    "        # 80 x 80 x 128\n",
    "        res_1_out = self.res_1(conv_3_out)\n",
    "        # 76 x 76 x 128\n",
    "        res_2_out = self.res_2(res_1_out)\n",
    "        # 72 x 72 x 128\n",
    "        res_3_out = self.res_3(res_2_out)\n",
    "        # 68 x 68 x 128\n",
    "        res_4_out = self.res_4(res_3_out)\n",
    "        # 64 x 64 x 128\n",
    "        res_5_out = self.res_5(res_4_out)\n",
    "        # 128 x 128 x 64\n",
    "        conv_4_out = self.conv_4(res_5_out)\n",
    "        # 256 x 256 x 32\n",
    "        conv_5_out = self.conv_5(conv_4_out)\n",
    "        # 256 x 256 x 3\n",
    "        conv_6_out = self.conv_6(conv_5_out)\n",
    "        tanh_out = tf.tanh(conv_6_out)\n",
    "        out = (tanh_out + 1) * 255. / 2\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=out)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_input(x):\n",
    "        \"\"\" Normalizes input\n",
    "        \"\"\"\n",
    "        return (x / 127.5) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try forward pass the content_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = StyleTransferModel()\n",
    "net(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_features(model, image):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of style layers.\n",
    "        Returns a dictionary of the layer name and the activations.\n",
    "    \"\"\"\n",
    "    style_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3']\n",
    "    \n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for layer in model.layers:\n",
    "        x = layer(x)\n",
    "        if layer.name in style_layers:\n",
    "            features[layer.name] = x\n",
    "            if layer.name == 'block4_conv3':\n",
    "                break\n",
    "            \n",
    "    return features\n",
    "\n",
    "def get_content_feature(model, image):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of conent layers.\n",
    "        Returns the activation of the content layer\n",
    "    \"\"\"\n",
    "    style_layers = ['block3_conv3']\n",
    "    \n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for layer in model.layers:\n",
    "        x = layer(x)\n",
    "        if layer.name in style_layers:\n",
    "            features = x\n",
    "            break\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    \"\"\" Compute gram matrix of a 3 dimensional convolution\n",
    "    \"\"\"\n",
    "    b, h, w, c = tf.shape(x)\n",
    "    x = tf.reshape(x, [b, c, -1])\n",
    "    size = tf.to_float(c * h * w)\n",
    "    return  tf.matmul(x, tf.transpose(x, perm=[0, 2, 1])) / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, content_image, style_image, content_weight, style_weight):\n",
    "    \"\"\" Compute loss of output with respect to content and style image\n",
    "    \"\"\"\n",
    "    # Pretrained VGG16 on imagenet\n",
    "    vgg = tf.keras.applications.vgg16.VGG16()\n",
    "    \n",
    "    # Freeze all VGG layers\n",
    "    for layer in vgg.layers[-19:]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Style features of output\n",
    "    output_style_features = get_style_features(vgg, y)\n",
    "    # Content features of output\n",
    "    output_content_feature = get_content_feature(vgg, y)\n",
    "    \n",
    "    # Style features of style image\n",
    "    style_features = get_style_features(vgg, style_image)\n",
    "    # Content features of content image\n",
    "    content_feature = get_content_feature(vgg, content_image)\n",
    "    \n",
    "    # Compute content loss\n",
    "    # (output - content )/(Cj * Hj * Wj)\n",
    "    content_loss = content_weight * tf.reduce_mean(tf.math.square(output_content_feature - content_feature))\n",
    "    \n",
    "    # Compute style loss\n",
    "    # Gram matrix of output features\n",
    "    output_grams = [gram_matrix(x) for _, x in output_style_features.items()]\n",
    "                                                  \n",
    "    # Gram matrix of style features\n",
    "    style_grams = [gram_matrix(x) for _, x in style_features.items()]\n",
    "    \n",
    "    style_losses = [tf.square(tf.norm(output_gram - style_gram)) for output_gram, style_gram in zip(output_grams, style_grams)]\n",
    "    style_loss = style_weight * tf.reduce_sum(tf.convert_to_tensor(style_losses)) / 4.\n",
    "                                                  \n",
    "    # TODO: Add total variation regularization\n",
    "    \n",
    "    total_loss = content_loss + style_loss\n",
    "    return total_loss, content_loss, style_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StyleTransferModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_WEIGHT = 7.5e0\n",
    "STYLE_WEIGHT = 1e2\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, style_image, content_weight=7.5e0, style_weight=1e2, optimizer=optimizer, global_step=global_step, num_epoch=2, print_every=1):\n",
    "    # keep results for plotting\n",
    "    train_loss_results = []\n",
    "\n",
    "    style_image_normalized = tf.keras.applications.vgg16.preprocess_input(style_image, mode='caffe')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training loop - using batches of 4\n",
    "        for image in train_dataset:\n",
    "            # Create the caffe normalized representation of content_image\n",
    "            content_image_normalized = tf.keras.applications.vgg16.preprocess_input(image, mode='caffe')\n",
    "\n",
    "            # Optimize the model\n",
    "            with tf.GradientTape() as tape: \n",
    "                y_ = model(image)\n",
    "                loss_value = loss(y_, content_image_normalized, style_image_normalized, CONTENT_WEIGHT, STYLE_WEIGHT)\n",
    "\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "            # Apply the gradients\n",
    "            optimizer.apply_gradients(zip(grads, model.variables), global_step)\n",
    "\n",
    "        # end epoch\n",
    "        train_loss_results.append(loss_value)\n",
    "\n",
    "        if global_step % print_every == 0:\n",
    "            print(\"Epoch {:03d}: Step: {:03d}... Loss: {:.3f}...\".format(epoch, global_step, loss_value))\n",
    "    \n",
    "    return model, train_loss_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(losses):\n",
    "    plt.ylabel(\"Loss\", fontsize=14)\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckpt(model, optimizer):\n",
    "    root = tf.train.Checkpoint(optimizer=optimizer, \n",
    "                               model=model, \n",
    "                               global_step=tf.train.get_or_create_global_step())\n",
    "    checkpoint_dir = 'models'\n",
    "    checkpoint_prefix = os.path.join(checkpoints_dir, 'ckpt')\n",
    "    root.save(checkpoints_prefix)\n",
    "    \n",
    "def load_ckpt(ckpt_dir='models'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    model = StyleTransferModel()\n",
    "    root = tf.train.Checkpoint(optimizer=optimizer, \n",
    "                               model=model, \n",
    "                               global_step=tf.train.get_or_create_global_step())\n",
    "    root.restore(tf.train.latest_checkpoint(ckpt_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, losses = train(model, style_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
