{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Style Transfer with TensorFlow and Keras\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll implement a network that performs __neural style transfer__ based on paper by [Justin Johnson, et al](https://cs.stanford.edu/people/jcjohns/eccv16/).\n",
    ">Using this method is giving similar qualitative results but is three orders of magnitude faster than optimization technique outlined in [Leon A. Gatys' paper, A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)\n",
    "\n",
    "## Overview\n",
    "Neural style transfer is an optimization technique used to take three images, a __content__ image, a __style reference__ image (such as an artwork by a famous painter), and the input image you want to style -- and blend them together such that the input image is transformed to look like the content image, but “painted” in the style of the style image.\n",
    "\n",
    "In this paper, style transfer is done by training a deep convolutional neural network using a pretrained deep convolutional neural network. In this case, we're using VGG16 pretrained on imagenet dataset.\n",
    "\n",
    "# TODO: masukin gambar arsitektur networknya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS:\n",
    "1. Create keras.Layers class instead of functions\n",
    "2. Search for style images\n",
    "\n",
    "### List of style images\n",
    "1. starry night\n",
    "2. hockney\n",
    "3. monet\n",
    "4. rain princess\n",
    "5. the scream\n",
    "6. udnie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Activation, add, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the Content and Style images\n",
    "Here, we create function to load image and do VGG16 standard preprocessing using `tf.keras.applications.vgg16.preprocess_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\" Load image from path and do VGG16 standard preprocessing.\n",
    "    \"\"\"\n",
    "    image = load_img(image_path)\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    image = tf.convert_to_tensor(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, title=None):\n",
    "    \"\"\" Showing image tensor\n",
    "    \"\"\"\n",
    "    image = image.numpy()\n",
    "    image = (image + 1.) * 127.5\n",
    "    # Remove the batch dimension\n",
    "    image = np.squeeze(image, axis=0)\n",
    "    print(image.shape)\n",
    "#     image = image.clip(0, 1)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = load_image(\"images/content/mug.jpg\")\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_WEIGHT = 7.5e0\n",
    "STYLE_WEIGHT = 1e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_zip = 'train2014.zip'\n",
    "name_of_folder = 'train2014'\n",
    "if not os.path.exists(os.path.abspath('.') + '/' + name_of_folder):\n",
    "  image_zip = tf.keras.utils.get_file(name_of_zip, \n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "  mscoco_path = os.path.dirname(image_zip)+'/train2014/'\n",
    "else:\n",
    "  mscoco_path = os.path.abspath('.')+'/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mscoco_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_padding():\n",
    "    \"\"\"Reflection padding layer for output size to match the input size\"\"\"\n",
    "    def f(inputs):\n",
    "        return tf.pad(inputs, [[0, 0], [40, 40], [40, 40], [0, 0]], \"REFLECT\")\n",
    "    return f\n",
    "\n",
    "def conv_layer(n_channels, kernel_size, strides, padding=\"same\", relu=True):\n",
    "    \"\"\"Convolutional layer wrapper\"\"\"\n",
    "    def f(inputs):\n",
    "        conv = Conv2D(filters=n_channels, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
    "        bn = BatchNormalization()(conv)\n",
    "        if relu:\n",
    "            return Activation(\"relu\")(bn)\n",
    "        else:\n",
    "            return bn\n",
    "        \n",
    "    return f\n",
    "\n",
    "def conv_transpose_layer(n_channels, kernel_size, strides, padding=\"same\", relu=True):\n",
    "    \"\"\"Convolutional transpose layer to upsample the image\"\"\"\n",
    "    def f(inputs):\n",
    "        conv = Conv2DTranspose(n_channels, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
    "        bn = BatchNormalization()(conv)\n",
    "        if relu:\n",
    "            return Activation(\"relu\")(bn)\n",
    "        else:\n",
    "            return bn\n",
    "        \n",
    "    return f\n",
    "\n",
    "def residual_block(n_channels, kernel_size=3, strides=1, padding='valid'):\n",
    "    \"\"\"Residual Block. Center cropped the input to match output size\"\"\"\n",
    "    def f(inputs):\n",
    "        inputs_shape = inputs.get_shape().as_list()\n",
    "        residual = tf.image.resize_image_with_crop_or_pad(inputs, inputs_shape[1] - 4, inputs_shape[2] - 4)\n",
    "        conv_1 = Conv2D(filters=n_channels, kernel_size=kernel_size, \n",
    "                      strides=strides, padding=padding)(inputs)\n",
    "        bn_1 = BatchNormalization()(conv_1)\n",
    "        relu_1 = Activation(\"relu\")(bn_1)\n",
    "        conv_2 = Conv2D(filters=n_channels, kernel_size=kernel_size, \n",
    "                      strides=strides, padding=padding)(relu_1)\n",
    "        bn_2 = BatchNormalization()(conv_2)\n",
    "        return add([bn_2, residual])\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferModel(tf.keras.Model):\n",
    "    \"\"\"Style Transfer Model class\"\"\"\n",
    "    def __init__(self):\n",
    "        super(StyleTransferModel, self).__init__(name='style_transfer_model')\n",
    "        \n",
    "        # Layers\n",
    "        self.pad = reflection_padding()\n",
    "        self.conv_1 = conv_layer(32, 9, 1)\n",
    "        self.conv_2 = conv_layer(64, 3, 2)\n",
    "        self.conv_3 = conv_layer(128, 3, 2)\n",
    "        self.res_1 = residual_block(128, 3, 1)\n",
    "        self.res_2 = residual_block(128, 3, 1)\n",
    "        self.res_3 = residual_block(128, 3, 1)\n",
    "        self.res_4 = residual_block(128, 3, 1)\n",
    "        self.res_5 = residual_block(128, 3, 1)\n",
    "        self.conv_4 = conv_transpose_layer(64, 3, 2)\n",
    "        self.conv_5 = conv_transpose_layer(32, 3, 2)\n",
    "        self.conv_6 = conv_layer(3, 9, 1, relu=False)\n",
    "\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # (width x height x channels)\n",
    "        # 256 x 256 x 3\n",
    "\n",
    "        # 336 x 336 x 3\n",
    "        padded = self.pad(inputs)\n",
    "        # 336 x 336 x 32\n",
    "        conv_1_out = self.conv_1(padded)\n",
    "        # 168 x 64 x 64\n",
    "        conv_2_out = self.conv_2(conv_1_out)\n",
    "        # 84 x 84 x 128\n",
    "        conv_3_out = self.conv_3(conv_2_out)\n",
    "        # 80 x 80 x 128\n",
    "        res_1_out = self.res_1(conv_3_out)\n",
    "        # 76 x 76 x 128\n",
    "        res_2_out = self.res_2(res_1_out)\n",
    "        # 72 x 72 x 128\n",
    "        res_3_out = self.res_3(res_2_out)\n",
    "        # 68 x 68 x 128\n",
    "        res_4_out = self.res_4(res_3_out)\n",
    "        # 64 x 64 x 128\n",
    "        res_5_out = self.res_5(res_4_out)\n",
    "        # 128 x 128 x 64\n",
    "        conv_4_out = self.conv_4(res_5_out)\n",
    "        # 256 x 256 x 32\n",
    "        conv_5_out = self.conv_5(conv_4_out)\n",
    "        # 256 x 256 x 3\n",
    "        conv_6_out = self.conv_6(conv_5_out)\n",
    "        \n",
    "        tanh_out = (conv_6_out + 1) * 255. / 2\n",
    "        return tanh_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = StyleTransferModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "content_image = load_img(\"images/content/mug.jpg\")\n",
    "content_image = img_to_array(content_image)\n",
    "plt.imshow(content_image/255.)\n",
    "plt.show()\n",
    "content_image = content_image.reshape((1, content_image.shape[0], content_image.shape[1], content_image.shape[2]))\n",
    "content_image = preprocess_input(content_image)\n",
    "content_image = tf.convert_to_tensor(content_image)\n",
    "tf.shape(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = load_img(\"images/styles/wave_crop.jpg\", target_size=(480, 640))\n",
    "style_image = img_to_array(style_image)\n",
    "plt.imshow(style_image/255.)\n",
    "plt.show()\n",
    "style_image = style_image.reshape((1, style_image.shape[0], style_image.shape[1], style_image.shape[2]))\n",
    "style_image = preprocess_input(style_image)\n",
    "style_image = tf.convert_to_tensor(style_image)\n",
    "tf.shape(style_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = net(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_features(model, image):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of style layers.\n",
    "        Returns a dictionary of the layer name and the activations.\n",
    "    \"\"\"\n",
    "    style_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3']\n",
    "    \n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for layer in model.layers:\n",
    "        x = layer(x)\n",
    "        if layer.name in style_layers:\n",
    "            features[layer.name] = x\n",
    "            if layer.name == 'block4_conv3':\n",
    "                break\n",
    "            \n",
    "    return features\n",
    "\n",
    "def get_content_feature(model, image):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of conent layers.\n",
    "        Returns the activation of the content layer\n",
    "    \"\"\"\n",
    "    style_layers = ['block3_conv3']\n",
    "    \n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for layer in model.layers:\n",
    "        x = layer(x)\n",
    "        if layer.name in style_layers:\n",
    "            features = x\n",
    "            break\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    \"\"\" Compute gram matrix of a 3 dimensional convolution\n",
    "    \"\"\"\n",
    "    b, h, w, c = tf.shape(x)\n",
    "    x = tf.reshape(x, [b, c, -1])\n",
    "    size = tf.to_float(c * h * w)\n",
    "    return  tf.matmul(x, tf.transpose(x, perm=[0, 2, 1])) / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, content_image, style_image, content_weight, style_weight):\n",
    "    \"\"\" Compute loss of output with respect to content and style image\n",
    "    \"\"\"\n",
    "    # Pretrained VGG16 on imagenet\n",
    "    model = VGG16()\n",
    "    \n",
    "    # Style features of output\n",
    "    output_style_features = get_style_features(model, y)\n",
    "    # Content features of output\n",
    "    output_content_feature = get_content_feature(model, y)\n",
    "    \n",
    "    # Style features of style image\n",
    "    style_features = get_style_features(model, style_image)\n",
    "    # Content features of content image\n",
    "    content_feature = get_content_feature(model, content_image)\n",
    "    \n",
    "    # Compute content loss\n",
    "    # (output - content )/(Cj * Hj * Wj)\n",
    "    content_loss = content_weight * tf.reduce_mean(tf.math.square(output_content_feature - content_feature))\n",
    "    \n",
    "    # Compute style loss\n",
    "    # Gram matrix of output features\n",
    "    output_grams = [gram_matrix(x) for _, x in output_style_features.items()]\n",
    "                                                  \n",
    "    # Gram matrix of style features\n",
    "    style_grams = [gram_matrix(x) for _, x in style_features.items()]\n",
    "    \n",
    "    style_losses = [tf.square(tf.norm(output_gram - style_gram)) for output_gram, style_gram in zip(output_grams, style_grams)]\n",
    "    style_loss = style_weight * tf.reduce_sum(tf.convert_to_tensor(style_losses)) / 4.\n",
    "                                                  \n",
    "    # TODO: Add total variation regularization\n",
    "    \n",
    "    total_loss = content_loss + style_loss\n",
    "    return total_loss, content_loss, style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tf.Tensor: id=18761, shape=(), dtype=float32, numpy=4519431000000.0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tf.Tensor: id=12538, shape=(), dtype=float32, numpy=2355534000000.0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(y_hat, content_image, style_image, 1, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
