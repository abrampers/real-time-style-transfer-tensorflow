{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Style Transfer with TensorFlow and Keras\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/abrampers/real-time-style-transfer-tensorflow/blob/master/Real%20Time%20Style%20Transfer%20-%20TensorFlow.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/abrampers/real-time-style-transfer-tensorflow/blob/master/Real%20Time%20Style%20Transfer%20-%20TensorFlow.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll implement a network that performs __neural style transfer__ based on paper by [Justin Johnson, et al](https://cs.stanford.edu/people/jcjohns/eccv16/).\n",
    ">Using this method is giving similar qualitative results but is three orders of magnitude faster than optimization technique outlined in [Leon A. Gatys' paper, A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)\n",
    "\n",
    "## Overview\n",
    "Neural style transfer is an optimization technique used to take three images, a __content__ image, a __style reference__ image (such as an artwork by a famous painter), and the input image you want to style -- and blend them together such that the input image is transformed to look like the content image, but “painted” in the style of the style image.\n",
    "\n",
    "In this paper, style transfer is done by training a deep convolutional neural network using a pretrained deep convolutional neural network. In this case, we're using [VGG16](https://arxiv.org/abs/1409.1556) pretrained on imagenet dataset.\n",
    "\n",
    "# TODO: masukin gambar arsitektur networknya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS:\n",
    "1. Create keras.Layers class instead of functions\n",
    "2. Search for style images\n",
    "\n",
    "### List of style images\n",
    "1. starry night\n",
    "2. hockney\n",
    "3. monet\n",
    "4. rain princess\n",
    "5. the scream\n",
    "6. udnie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Activation, add, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the Content and Style images\n",
    "Here, we create function to load image and do VGG16 standard preprocessing using `tf.keras.applications.vgg16.preprocess_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, target_size=None):\n",
    "    \"\"\" Load image from path and do VGG16 standard preprocessing with tf mode.\n",
    "        Returns a tensor representation of the image.\n",
    "    \"\"\"\n",
    "    if target_size is None:\n",
    "        image = load_img(image_path)\n",
    "    else:\n",
    "        image = load_img(image_path, target_size=target_size)\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image, mode='tf')\n",
    "    image = tf.convert_to_tensor(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, title=None):\n",
    "    \"\"\" Showing image tensor\n",
    "    \"\"\"\n",
    "    image = image.numpy()\n",
    "    image = (image + 1.) * 127.5\n",
    "    # Remove the batch dimension\n",
    "    image = np.squeeze(image, axis=0)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.imshow(image/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = load_image(\"images/content/mug.jpg\")\n",
    "imshow(content_image)\n",
    "content_shape = tf.shape(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = load_image(\"images/styles/hockney.jpg\", target_size=(content_shape[1], content_shape[2]))\n",
    "imshow(style_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape(style_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_WEIGHT = 7.5e0\n",
    "STYLE_WEIGHT = 1e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_zip = 'train2014.zip'\n",
    "name_of_folder = 'train2014'\n",
    "if not os.path.exists(os.path.abspath('.') + '/' + name_of_folder):\n",
    "  image_zip = tf.keras.utils.get_file(name_of_zip, \n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "  mscoco_path = os.path.dirname(image_zip)+'/train2014/'\n",
    "else:\n",
    "  mscoco_path = os.path.abspath('.')+'/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mscoco_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_padding():\n",
    "    \"\"\"Reflection padding layer for output size to match the input size\"\"\"\n",
    "    def f(inputs):\n",
    "        return tf.pad(inputs, [[0, 0], [40, 40], [40, 40], [0, 0]], \"REFLECT\")\n",
    "    return f\n",
    "\n",
    "def conv_layer(n_channels, kernel_size, strides, padding=\"same\", relu=True):\n",
    "    \"\"\"Convolutional layer wrapper\"\"\"\n",
    "    def f(inputs):\n",
    "        conv = Conv2D(filters=n_channels, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
    "        bn = BatchNormalization()(conv)\n",
    "        if relu:\n",
    "            return Activation(\"relu\")(bn)\n",
    "        else:\n",
    "            return bn\n",
    "        \n",
    "    return f\n",
    "\n",
    "def conv_transpose_layer(n_channels, kernel_size, strides, padding=\"same\", relu=True):\n",
    "    \"\"\"Convolutional transpose layer to upsample the image\"\"\"\n",
    "    def f(inputs):\n",
    "        conv = Conv2DTranspose(n_channels, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
    "        bn = BatchNormalization()(conv)\n",
    "        if relu:\n",
    "            return Activation(\"relu\")(bn)\n",
    "        else:\n",
    "            return bn\n",
    "        \n",
    "    return f\n",
    "\n",
    "def residual_block(n_channels, kernel_size=3, strides=1, padding='valid'):\n",
    "    \"\"\"Residual Block. Center cropped the input to match output size\"\"\"\n",
    "    def f(inputs):\n",
    "        inputs_shape = inputs.get_shape().as_list()\n",
    "        residual = tf.image.resize_image_with_crop_or_pad(inputs, inputs_shape[1] - 4, inputs_shape[2] - 4)\n",
    "        conv_1 = Conv2D(filters=n_channels, kernel_size=kernel_size, \n",
    "                      strides=strides, padding=padding)(inputs)\n",
    "        bn_1 = BatchNormalization()(conv_1)\n",
    "        relu_1 = Activation(\"relu\")(bn_1)\n",
    "        conv_2 = Conv2D(filters=n_channels, kernel_size=kernel_size, \n",
    "                      strides=strides, padding=padding)(relu_1)\n",
    "        bn_2 = BatchNormalization()(conv_2)\n",
    "        return add([bn_2, residual])\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferModel(tf.keras.Model):\n",
    "    \"\"\"Style Transfer Model class\"\"\"\n",
    "    def __init__(self):\n",
    "        super(StyleTransferModel, self).__init__(name='style_transfer_model')\n",
    "        \n",
    "        # Layers\n",
    "        self.pad = reflection_padding()\n",
    "        self.conv_1 = conv_layer(32, 9, 1)\n",
    "        self.conv_2 = conv_layer(64, 3, 2)\n",
    "        self.conv_3 = conv_layer(128, 3, 2)\n",
    "        self.res_1 = residual_block(128, 3, 1)\n",
    "        self.res_2 = residual_block(128, 3, 1)\n",
    "        self.res_3 = residual_block(128, 3, 1)\n",
    "        self.res_4 = residual_block(128, 3, 1)\n",
    "        self.res_5 = residual_block(128, 3, 1)\n",
    "        self.conv_4 = conv_transpose_layer(64, 3, 2)\n",
    "        self.conv_5 = conv_transpose_layer(32, 3, 2)\n",
    "        self.conv_6 = conv_layer(3, 9, 1, relu=False)\n",
    "\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # (width x height x channels)\n",
    "        # 256 x 256 x 3\n",
    "\n",
    "        # 336 x 336 x 3\n",
    "        padded = self.pad(inputs)\n",
    "        # 336 x 336 x 32\n",
    "        conv_1_out = self.conv_1(padded)\n",
    "        # 168 x 64 x 64\n",
    "        conv_2_out = self.conv_2(conv_1_out)\n",
    "        # 84 x 84 x 128\n",
    "        conv_3_out = self.conv_3(conv_2_out)\n",
    "        # 80 x 80 x 128\n",
    "        res_1_out = self.res_1(conv_3_out)\n",
    "        # 76 x 76 x 128\n",
    "        res_2_out = self.res_2(res_1_out)\n",
    "        # 72 x 72 x 128\n",
    "        res_3_out = self.res_3(res_2_out)\n",
    "        # 68 x 68 x 128\n",
    "        res_4_out = self.res_4(res_3_out)\n",
    "        # 64 x 64 x 128\n",
    "        res_5_out = self.res_5(res_4_out)\n",
    "        # 128 x 128 x 64\n",
    "        conv_4_out = self.conv_4(res_5_out)\n",
    "        # 256 x 256 x 32\n",
    "        conv_5_out = self.conv_5(conv_4_out)\n",
    "        # 256 x 256 x 3\n",
    "        conv_6_out = self.conv_6(conv_5_out)\n",
    "        \n",
    "        tanh_out = (conv_6_out + 1) * 255. / 2\n",
    "        return tanh_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = StyleTransferModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "content_image = load_img(\"images/content/mug.jpg\")\n",
    "content_image = img_to_array(content_image)\n",
    "plt.imshow(content_image/255.)\n",
    "plt.show()\n",
    "content_image = content_image.reshape((1, content_image.shape[0], content_image.shape[1], content_image.shape[2]))\n",
    "content_image = preprocess_input(content_image)\n",
    "content_image = tf.convert_to_tensor(content_image)\n",
    "tf.shape(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = load_img(\"images/styles/wave_crop.jpg\", target_size=(480, 640))\n",
    "style_image = img_to_array(style_image)\n",
    "plt.imshow(style_image/255.)\n",
    "plt.show()\n",
    "style_image = style_image.reshape((1, style_image.shape[0], style_image.shape[1], style_image.shape[2]))\n",
    "style_image = preprocess_input(style_image)\n",
    "style_image = tf.convert_to_tensor(style_image)\n",
    "tf.shape(style_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = net(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_features(model, image):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of style layers.\n",
    "        Returns a dictionary of the layer name and the activations.\n",
    "    \"\"\"\n",
    "    style_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3']\n",
    "    \n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for layer in model.layers:\n",
    "        x = layer(x)\n",
    "        if layer.name in style_layers:\n",
    "            features[layer.name] = x\n",
    "            if layer.name == 'block4_conv3':\n",
    "                break\n",
    "            \n",
    "    return features\n",
    "\n",
    "def get_content_feature(model, image):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of conent layers.\n",
    "        Returns the activation of the content layer\n",
    "    \"\"\"\n",
    "    style_layers = ['block3_conv3']\n",
    "    \n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for layer in model.layers:\n",
    "        x = layer(x)\n",
    "        if layer.name in style_layers:\n",
    "            features = x\n",
    "            break\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    \"\"\" Compute gram matrix of a 3 dimensional convolution\n",
    "    \"\"\"\n",
    "    b, h, w, c = tf.shape(x)\n",
    "    x = tf.reshape(x, [b, c, -1])\n",
    "    size = tf.to_float(c * h * w)\n",
    "    return  tf.matmul(x, tf.transpose(x, perm=[0, 2, 1])) / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, content_image, style_image, content_weight, style_weight):\n",
    "    \"\"\" Compute loss of output with respect to content and style image\n",
    "    \"\"\"\n",
    "    # Pretrained VGG16 on imagenet\n",
    "    model = VGG16()\n",
    "    \n",
    "    # Style features of output\n",
    "    output_style_features = get_style_features(model, y)\n",
    "    # Content features of output\n",
    "    output_content_feature = get_content_feature(model, y)\n",
    "    \n",
    "    # Style features of style image\n",
    "    style_features = get_style_features(model, style_image)\n",
    "    # Content features of content image\n",
    "    content_feature = get_content_feature(model, content_image)\n",
    "    \n",
    "    # Compute content loss\n",
    "    # (output - content )/(Cj * Hj * Wj)\n",
    "    content_loss = content_weight * tf.reduce_mean(tf.math.square(output_content_feature - content_feature))\n",
    "    \n",
    "    # Compute style loss\n",
    "    # Gram matrix of output features\n",
    "    output_grams = [gram_matrix(x) for _, x in output_style_features.items()]\n",
    "                                                  \n",
    "    # Gram matrix of style features\n",
    "    style_grams = [gram_matrix(x) for _, x in style_features.items()]\n",
    "    \n",
    "    style_losses = [tf.square(tf.norm(output_gram - style_gram)) for output_gram, style_gram in zip(output_grams, style_grams)]\n",
    "    style_loss = style_weight * tf.reduce_sum(tf.convert_to_tensor(style_losses)) / 4.\n",
    "                                                  \n",
    "    # TODO: Add total variation regularization\n",
    "    \n",
    "    total_loss = content_loss + style_loss\n",
    "    return total_loss, content_loss, style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tf.Tensor: id=18761, shape=(), dtype=float32, numpy=4519431000000.0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tf.Tensor: id=12538, shape=(), dtype=float32, numpy=2355534000000.0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(y_hat, content_image, style_image, 1, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
